{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不定长序列pad 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)  # 并行gpu\n",
    "        torch.backends.cudnn.deterministic = True  # cpu/gpu结果一致\n",
    "        torch.backends.cudnn.benchmark = True  # 训练集变化不大时使训练加速\n",
    "        \n",
    "# 随机种子\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "参考资料:\n",
    "https://www.cnblogs.com/sbj123456789/p/9834018.html\n",
    "对于序列长度可变的情况，介绍使用pad_sequence函数的用法\n",
    "对于序列不等长的情况，使用填充0来进行等长序列的操作，0的填充可能需要知道所有数据的最大长度，然后开始填充，这样不是很合理，因为\n",
    "按照批次进行的话，我们希望得到每个批次里面最大的长度即可，然后开始进行填充。使用pad_sequence 函数里面的collate_fn函数来进行操作即可\n",
    "\"\"\"\n",
    "train_x = [torch.FloatTensor([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
    "           torch.FloatTensor([1, 2, 3, 4, 5, 6, 7]),\n",
    "           torch.FloatTensor([2, 3, 4, 5, 6, 7]),\n",
    "           torch.FloatTensor([3, 4, 5, 6, 7]),\n",
    "           torch.FloatTensor([4, 5, 6, 7]),\n",
    "           torch.FloatTensor([5, 6, 7]),\n",
    "           torch.FloatTensor([6, 7]),\n",
    "           torch.FloatTensor([7])]  # 数据类型是浮点型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(data.Dataset):\n",
    "    \"\"\"\n",
    "    这里什么都不做，把数据补0功能放到collate_fn这个函数里面去\n",
    "    \"\"\"\n",
    "    def __init__(self, train_x):\n",
    "        self.train_x = train_x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_x)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.train_x[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(train_data):\n",
    "    \"\"\"\n",
    "    该函数的功能就是对train_data数据进行填充，填充原则是对当前批次的数据长度先要进行排序\n",
    "    按照从大到小的顺序排序，然后开始填充\n",
    "    :param train_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_data.sort(key=lambda data: len(data), reverse=True)  # 按照长度排序\n",
    "    data_length = [len(data) for data in train_data]  # 得到排序后的数据的长度列表\n",
    "    train_data = torch.nn.utils.rnn.pad_sequence(train_data, batch_first=True, padding_value=0)  # 对该数据进行填充\n",
    "    return train_data.unsqueeze(-1), data_length  # 对train_data增加了一维数据，返回数据和长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data_ is: \n",
      " PackedSequence(data=tensor([[1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]]), batch_sizes=tensor([2, 2, 2, 2, 2, 2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "output is:  PackedSequence(data=tensor([[ 0.1052, -0.1472,  0.0265,  0.1616,  0.2554],\n",
      "        [ 0.1052, -0.1472,  0.0265,  0.1616,  0.2554],\n",
      "        [ 0.2213, -0.2098,  0.0769,  0.2237,  0.4416],\n",
      "        [ 0.2213, -0.2098,  0.0769,  0.2237,  0.4416],\n",
      "        [ 0.3324, -0.2415,  0.1375,  0.2272,  0.5453],\n",
      "        [ 0.3324, -0.2415,  0.1375,  0.2272,  0.5453],\n",
      "        [ 0.4152, -0.2645,  0.1953,  0.1752,  0.5986],\n",
      "        [ 0.4152, -0.2645,  0.1953,  0.1752,  0.5986],\n",
      "        [ 0.4578, -0.2880,  0.2404,  0.0654,  0.6206],\n",
      "        [ 0.4578, -0.2880,  0.2404,  0.0654,  0.6206],\n",
      "        [ 0.4656, -0.3140,  0.2682, -0.0961,  0.6211],\n",
      "        [ 0.4656, -0.3140,  0.2682, -0.0961,  0.6211],\n",
      "        [ 0.4523, -0.3416,  0.2794, -0.2880,  0.6060],\n",
      "        [ 0.4523, -0.3416,  0.2794, -0.2880,  0.6060],\n",
      "        [ 0.4294, -0.3687,  0.2803, -0.4752,  0.5803],\n",
      "        [ 0.4047, -0.3934,  0.2807, -0.6280,  0.5496]], grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 2, 2, 2, 2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([9, 2, 5])\n",
      "the data_ is: \n",
      " PackedSequence(data=tensor([[2.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [7.]]), batch_sizes=tensor([2, 2, 2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "output is:  PackedSequence(data=tensor([[ 0.1539, -0.1861,  0.0533,  0.1609,  0.3283],\n",
      "        [ 0.1928, -0.2233,  0.0887,  0.1369,  0.3771],\n",
      "        [ 0.2998, -0.2413,  0.1215,  0.1974,  0.4994],\n",
      "        [ 0.3544, -0.2691,  0.1679,  0.1305,  0.5304],\n",
      "        [ 0.4013, -0.2664,  0.1859,  0.1581,  0.5783],\n",
      "        [ 0.4346, -0.2915,  0.2262,  0.0386,  0.5901],\n",
      "        [ 0.4521, -0.2891,  0.2352,  0.0542,  0.6111],\n",
      "        [ 0.4569, -0.3157,  0.2605, -0.1140,  0.6068],\n",
      "        [ 0.4631, -0.3146,  0.2651, -0.1037,  0.6164],\n",
      "        [ 0.4487, -0.3425,  0.2747, -0.2994,  0.5991],\n",
      "        [ 0.4511, -0.3420,  0.2774, -0.2928,  0.6036]], grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([6, 2, 5])\n",
      "the data_ is: \n",
      " PackedSequence(data=tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [7.]]), batch_sizes=tensor([2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "output is:  PackedSequence(data=tensor([[ 0.2206, -0.2581,  0.1316,  0.0902,  0.4070],\n",
      "        [ 0.2391, -0.2898,  0.1805,  0.0246,  0.4236],\n",
      "        [ 0.3848, -0.2943,  0.2122,  0.0242,  0.5436],\n",
      "        [ 0.3975, -0.3176,  0.2521, -0.1108,  0.5455],\n",
      "        [ 0.4415, -0.3171,  0.2556, -0.1216,  0.5877],\n",
      "        [ 0.4333, -0.3424,  0.2758, -0.2981,  0.5760],\n",
      "        [ 0.4440, -0.3429,  0.2728, -0.3046,  0.5912]], grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([4, 2, 5])\n",
      "the data_ is: \n",
      " PackedSequence(data=tensor([[6.],\n",
      "        [7.],\n",
      "        [7.]]), batch_sizes=tensor([2, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "output is:  PackedSequence(data=tensor([[ 0.2505, -0.3183,  0.2329, -0.0537,  0.4316],\n",
      "        [ 0.2568, -0.3436,  0.2867, -0.1381,  0.4339],\n",
      "        [ 0.3990, -0.3391,  0.2875, -0.2563,  0.5404]], grad_fn=<CatBackward0>), batch_sizes=tensor([2, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([2, 2, 5])\n",
      "torch.Size([2, 6, 10])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([8, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# 定义网络\n",
    "net = torch.nn.LSTM(1, 5, batch_first=True)\n",
    "\n",
    "train_data = MyData(train_x)\n",
    "\n",
    " # 进行数据处理\n",
    "train_dataloader = data.DataLoader(train_data, batch_size=2, collate_fn=collate_fn)\n",
    "for data_input, length in train_dataloader:\n",
    "    # 对于之前的加0操作的数据进行压缩，然后直接丢给lstm进行运算\n",
    "    data_ = torch.nn.utils.rnn.pack_padded_sequence(data_input, length, batch_first=True)\n",
    "    print('the data_ is: \\n', data_)\n",
    "    output, (ht, ct) = net(data_)\n",
    "    print(\"output is: \", output)\n",
    "    # 预算结果再解压缩补0然后提取结果\n",
    "    output_, _ = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
    "    print(output_.shape)\n",
    "\n",
    "\n",
    "def blstm():\n",
    "    lstm = torch.nn.LSTM(input_size=3, hidden_size=5, num_layers=4, batch_first=True, bidirectional=True)\n",
    "    x = torch.rand(2, 6, 3)\n",
    "    hidden_state = torch.zeros(4 * 2, 2, 5)\n",
    "    cell_state = torch.zeros(8, 2, 5)\n",
    "    outputs, (hidden_state, cell_state) = lstm(x, (hidden_state, cell_state))\n",
    "\n",
    "    print(outputs.shape)\n",
    "    print(hidden_state[-1, :, :].shape)\n",
    "    print(cell_state.shape)\n",
    "    # print(outputs)\n",
    "blstm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
