{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# pytorch基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.0 随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda3/envs/pytorch/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "\n",
    "# 设置随机种子\n",
    "def randomSeed(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "randomSeed(SEED)\n",
    "# hyper parameters\n",
    "in_dim = 1\n",
    "n_hidden_1 = 1\n",
    "n_hidden_2 = 1\n",
    "out_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.0 module 和children 区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================children=====================\n",
      "0 Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n",
      "1 Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n",
      "2 Linear(in_features=1, out_features=1, bias=True)\n",
      "=====================modules======================\n",
      "0 Net(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer3): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "1 Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n",
      "2 Linear(in_features=1, out_features=1, bias=True)\n",
      "3 ReLU(inplace=True)\n",
      "4 Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n",
      "5 Linear(in_features=1, out_features=1, bias=True)\n",
      "6 ReLU(inplace=True)\n",
      "7 Linear(in_features=1, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "in_dim = 1\n",
    "n_hidden_1 = 1\n",
    "n_hidden_2 = 1\n",
    "out_dim = 1\n",
    "\"\"\"\n",
    "module 和 children区别\n",
    "module 是深度优先遍历打印出网络结构,而 children是只打印出网络的子结构,不再管子结构的下一结构\n",
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, n_hidden_1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_1, n_hidden_2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.layer3 = nn.Linear(n_hidden_2, out_dim)\n",
    "\n",
    "        print(\"{:=^50s}\".format(\"children\"))\n",
    "        for i, module in enumerate(self.children()):\n",
    "            print(i, module)\n",
    "\n",
    "        print(\"{:=^50s}\".format(\"modules\"))        \n",
    "        for i, module in enumerate(self.modules()):\n",
    "            print(i, module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net(in_dim, n_hidden_1, n_hidden_2, out_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.0 向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "embedding vector\n",
    "\"\"\"\n",
    "word_to_ix = {'hello': 1, 'world': 2}\n",
    "# padding_idx (python:int, optional) – 填充id，比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，\n",
    "# 而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。（初始化为0）\n",
    "embeds = nn.Embedding(7, 5, padding_idx=0)\n",
    "hello_idx = torch.LongTensor([word_to_ix['hello']])\n",
    "hello_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2504,  0.2037, -0.6473,  1.2173,  0.6089]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 5])\n",
      "tensor([[-0.2504,  0.2037, -0.6473,  1.2173,  0.6089],\n",
      "        [-0.0959,  0.9456, -1.2708,  0.2959,  0.1503],\n",
      "        [ 0.4238, -0.6420,  1.2667, -0.0799, -0.3857]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "hello_embed = embeds(hello_idx)\n",
    "print(hello_embed)\n",
    "print(hello_embed.shape)\n",
    "\n",
    "# 1,3,5维度\n",
    "print(embeds(torch.LongTensor([1, 4, 3])))\n",
    "\n",
    "# 2,3,5\n",
    "print(embeds(torch.LongTensor([[1, 4, 3], [2, 3, 1]])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 5])\n",
      "embedding is  Embedding(2, 3)\n",
      "tensor([[4.0000, 5.1000, 6.3000]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randint(1, 7, (3, 3))\n",
    "print(embeds(inputs).shape)\n",
    "# print(embeds(inputs))\n",
    "\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding = nn.Embedding.from_pretrained(weight)\n",
    "print('embedding is ',embedding)\n",
    "input = torch.LongTensor([1])\n",
    "print(embedding(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.0 常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对位相乘, a 和 b 维度必须相同， mul is:\n",
      " tensor([[2., 6.],\n",
      "        [4., 9.]])\n",
      "矩阵乘法 mm is:\n",
      " tensor([[ 6.,  9.],\n",
      "        [10., 15.]])\n",
      "===================广播乘法 matmul==================== \n",
      "tensor([[ 6.,  9.],\n",
      "        [10., 15.]])\n",
      "一维点乘 dot is:\n",
      " tensor(14.)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.mm  矩阵乘法\n",
    "torch.bmm 三维矩阵乘法，第一维是batch size，比如[3,4,5] * [3,5,6] = [3,4,6]\n",
    "torch.matmul 广播乘法,1维的话返回标量,2维的话返回矩阵乘法结果,其他情况比较复杂,具有广播功能\n",
    "torch.mul 对位相乘,两个矩阵的维度必须一致才可以\n",
    "torch.dot\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "a = torch.Tensor([[1, 2], [2, 3]])\n",
    "b = torch.Tensor([[2, 3], [2, 3]])\n",
    "c = torch.mul(a, b)\n",
    "print('对位相乘, a 和 b 维度必须相同， mul is:\\n', c)\n",
    "d = torch.mm(a, b)\n",
    "print(\"矩阵乘法 mm is:\\n\", d)\n",
    "e = torch.matmul(a, b)\n",
    "\n",
    "print(\"{:=^50s} \\n{}\".format(\"广播乘法 matmul\", e))\n",
    "\n",
    "\n",
    "a1 = torch.Tensor([1, 2, 3])\n",
    "a2 = torch.Tensor([1, 2, 3])\n",
    "f = torch.dot(a1, a2)\n",
    "print('一维点乘 dot is:\\n', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False,  True])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.eq函数\n",
    "\"\"\"\n",
    "a = torch.FloatTensor([1, 2, 3])\n",
    "b = torch.FloatTensor([2, 21, 3])\n",
    "print(b.eq(a.data).cpu())\n",
    "# output is [false, false, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Train Loss: 0.880 | Train PPL:   2.410\n",
      "Train loss: 0.880 | train ppl:   2.410\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "print 格式化输出\n",
    "\"\"\"\n",
    "import math\n",
    "print('{0:n}'.format(20.000))\n",
    "train_loss = 0.8797\n",
    "print(f'Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')  # 7 前面是空格\n",
    "print('Train loss: {:.3f} | train ppl: {:7.3f}'.format(train_loss, math.exp(train_loss)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-inf, -inf, 3., 4.])\n",
      "tensor([0.0000, 0.0000, 0.2689, 0.7311])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1640811914285/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1273.)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "masked_fill， 使用mask 技巧\n",
    "\"\"\"\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "\n",
    "a = torch.Tensor([1, 2, 3, 4])\n",
    "a = a.masked_fill(mask=torch.ByteTensor([1, 1, 0, 0]), value=-np.inf)\n",
    "print(a)\n",
    "b = F.softmax(a, dim=0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-inf\n",
      "##########\n",
      "tensor([[[5, 5, 5, 5],\n",
      "         [6, 6, 6, 6],\n",
      "         [7, 7, 7, 7]],\n",
      "\n",
      "        [[1, 1, 1, 1],\n",
      "         [2, 2, 2, 2],\n",
      "         [3, 3, 3, 3]]])\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 1, 1])\n",
      "tensor([[[          5,           5,           5,           5],\n",
      "         [          6,           6,           6,           6],\n",
      "         [          7,           7,           7,           7]],\n",
      "\n",
      "        [[-1000000000, -1000000000, -1000000000, -1000000000],\n",
      "         [-1000000000, -1000000000, -1000000000, -1000000000],\n",
      "         [-1000000000, -1000000000, -1000000000, -1000000000]]])\n",
      "torch.Size([2, 3, 4])\n",
      "##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1640811914285/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1273.)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(-np.inf)\n",
    "print(\"#\"*10)\n",
    "a = torch.tensor([[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7]], [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]])\n",
    "print(a)\n",
    "print(a.size())\n",
    "mask = torch.ByteTensor([[[0]], [[1]]])\n",
    "print(mask.size())\n",
    "b = a.masked_fill(mask, value=torch.tensor(-1e9))\n",
    "print(b)\n",
    "print(b.size())\n",
    "print(\"#\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5, 5, 5, 5],\n",
      "         [6, 6, 6, 6],\n",
      "         [7, 7, 7, 7]],\n",
      "\n",
      "        [[1, 1, 1, 1],\n",
      "         [2, 2, 2, 2],\n",
      "         [3, 3, 3, 3]]])\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1640811914285/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1273.)\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor c is:\\ntensor([[[          5,           5,           5,           5],\\n         [          6,           6,           6,           6],\\n         [-1000000000, -1000000000, -1000000000, -1000000000]],\\n\\n        [[-1000000000, -1000000000, -1000000000, -1000000000],\\n         [          2,           2,           2,           2],\\n         [          3,           3,           3,           3]]])\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7]], [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]])\n",
    "print(a)\n",
    "print(a.size())\n",
    "mask = torch.ByteTensor([[[1], [1], [0]], [[0], [1], [1]]])\n",
    "print(mask.size())\n",
    "b = a.masked_fill(mask, value=torch.tensor(-1e9))\n",
    "\"\"\"\n",
    "tensor b is:\n",
    "tensor([[[-1000000000, -1000000000, -1000000000, -1000000000],\n",
    "         [-1000000000, -1000000000, -1000000000, -1000000000],\n",
    "         [          7,           7,           7,           7]],\n",
    "\n",
    "        [[          1,           1,           1,           1],\n",
    "         [-1000000000, -1000000000, -1000000000, -1000000000],\n",
    "         [-1000000000, -1000000000, -1000000000, -1000000000]]])\n",
    "\"\"\"\n",
    "c = a.masked_fill(mask == 0, value=torch.tensor(-1e9))  # mask等于0的对应的行列元素赋值为value\n",
    "\"\"\"\n",
    "tensor c is:\n",
    "tensor([[[          5,           5,           5,           5],\n",
    "         [          6,           6,           6,           6],\n",
    "         [-1000000000, -1000000000, -1000000000, -1000000000]],\n",
    "\n",
    "        [[-1000000000, -1000000000, -1000000000, -1000000000],\n",
    "         [          2,           2,           2,           2],\n",
    "         [          3,           3,           3,           3]]])\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "'''转换向量过程'''\n",
    "\"\"\"\n",
    "repeat 和 expand 两个函数的区别, 不会分配新内存,只是创建一个新的视图, 且只能扩展维度是1的张量\n",
    "\"\"\"\n",
    "a = torch.tensor([1, 2, 3, 4])\n",
    "a1 = a.expand(8, 4)  # 不会分配新内存,只是创建一个新的视图,且只能扩展维度是1的张量\n",
    "print(a1.shape)  # [8, 4]\n",
    "\n",
    "b1 = a.repeat(3, 2)  # 沿着特定维度重复张量\n",
    "print(b1.shape)  # shape =[3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.25 0.5  0.75 1.  ]\n",
      " [0.   0.25 0.5  0.75 1.  ]\n",
      " [0.   0.25 0.5  0.75 1.  ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.5 0.5 0.5 0.5 0.5]\n",
      " [1.  1.  1.  1.  1. ]]\n",
      "[[0.   0.  ]\n",
      " [0.25 0.  ]\n",
      " [0.5  0.  ]\n",
      " [0.75 0.  ]\n",
      " [1.   0.  ]\n",
      " [0.   0.5 ]\n",
      " [0.25 0.5 ]\n",
      " [0.5  0.5 ]\n",
      " [0.75 0.5 ]\n",
      " [1.   0.5 ]\n",
      " [0.   1.  ]\n",
      " [0.25 1.  ]\n",
      " [0.5  1.  ]\n",
      " [0.75 1.  ]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "meshgrid函数使用,构成一个坐标系可用,选择两个行列坐标的值,然后最后生成的坐标\n",
    "x和y轴的参数mn分别是较小的值作为行,大的作为列\n",
    "\"\"\"\n",
    "x = np.linspace(0, 1, 5)\n",
    "y = np.linspace(0, 1, 3)\n",
    "xc, yc = np.meshgrid(x, y)\n",
    "print(xc)\n",
    "print(yc)\n",
    "xc = xc.reshape(-1, 1)\n",
    "yc = yc.reshape(-1, 1)\n",
    "c = np.concatenate((xc, yc), axis=1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0930,  0.2338, -1.1586],\n",
      "        [-0.6521, -1.0775,  0.1823]])\n",
      "tensor([[ 0.,  0., -2.],\n",
      "        [-1., -2.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "torch.floor不超过这个数的最大整数\n",
    "tensor([[ 0.0461,  0.4024, -1.0115],\n",
    "        [ 0.2167, -0.6123,  0.5036]])\n",
    "tensor([[ 0.,  0., -2.],\n",
    "        [ 0., -1.,  0.]])\n",
    "\"\"\"\n",
    "a = torch.randn((2, 3))\n",
    "b = torch.floor(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 6, 7])\n",
      "tensor([[[[1., 1., 2., 2.],\n",
      "          [1., 1., 2., 2.],\n",
      "          [3., 3., 4., 4.],\n",
      "          [3., 3., 4., 4.]]]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "上采样函数\n",
    "nn.ConvTranspose2d,有参数可以训练\n",
    "hout = (hin-1)*stride - 2 * padding + kernel + output_padding\n",
    "参考链接https://blog.csdn.net/qq_27261889/article/details/86304061\n",
    "\n",
    "nn.Unsample 上采样没有参数,速度更快,采样策略给定\n",
    "\n",
    "参考链接 \n",
    "https://www.shuzhiduo.com/A/gGdX9OPWz4/\n",
    "https://blog.csdn.net/wangweiwells/article/details/101820932\n",
    "https://zhuanlan.zhihu.com/p/87572724(详解 align_corners=False, True用法)\n",
    "\n",
    "双线性插值算法\n",
    "https://juejin.im/post/6844903924999127047\n",
    "https://zhuanlan.zhihu.com/p/110754637\n",
    "\"\"\"\n",
    "input = torch.ones((2, 2, 3, 4))\n",
    "output = nn.ConvTranspose2d(2, 4, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "# [2, 4, 6, 7]\n",
    "print(output(input).shape)\n",
    "\n",
    "input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)\n",
    "m = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "output = m(input)\n",
    "print(output)\n",
    "m = nn.Upsample(scale_factor=2, mode='bilinear',align_corners=False)\n",
    "output = m(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}